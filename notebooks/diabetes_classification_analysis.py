# -*- coding: utf-8 -*-
"""Final Version_ Group 2B -  Project 2 Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PveW_qAaF1qb7A6BEvUE2jyPhAjZFlKI

# Environment Setup
"""

# import modules

import pandas as pd # for data viz and wrangling
import numpy as np # for 'numeric python'
import matplotlib.pyplot as plt # for data viz (more complex than pylab)
import seaborn as sns
from pylab import * # for data viz (import * means 'import all of the functions')
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import f_regression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn import metrics
from scipy import stats
import statsmodels.api as sm

# mount your google drive
from google.colab import drive
drive.mount('/content/drive')

"""# Load the Data"""

# read data
# on the lefthand side, navigate to your data and copy the path
df = pd.read_csv('/content/drive/MyDrive/diabetes_binary_health_indicators_BRFSS2015.csv')

# Preview
print(df.head())

unique_values = {}
for col in df.columns:
    unique_values[col] = df[col].value_counts().shape[0]

pd.DataFrame(unique_values, index=['unique value count']).transpose()

print(df.info())

"""Several columns are displayed with 0 and 1 indicators, so we converted them to the object data type."""

columns_to_keep_numeric = ['BMI', 'MentHlth', 'PhysHlth']

for col in df.columns:
    if col not in columns_to_keep_numeric:
        df[col] = df[col].astype(str)

display(df.info())

# shape
# shows how many rows and columns
# this sample has 253680 rows and 22 columns
df.shape

# distribution of target variable
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(6, 4))
ax = sns.countplot(x='Diabetes_binary', data=df)
plt.title('Distribution of Diagnosed Diabetes')
plt.xlabel('Diagnosed Diabetes (0: No, 1: Yes)')
plt.ylabel('Count')
# Add counts and percentages on top of bars
total = len(df)
for container in ax.containers:
    counts = container.datavalues if hasattr(container, 'datavalues') else [p.get_height() for p in container.patches]
    for count, patch in zip(counts, container.patches):
        percentage = count / total * 100
        ax.text(
            patch.get_x() + patch.get_width() / 2,
            patch.get_height(),
            f'{count}\n{percentage:.1f}%',
            ha='center',
            va='bottom'
        )
plt.ylim(0, 250000)
plt.tight_layout()
plt.show()

"""# Preprocessing"""

# Distribution of categorical variables
# Identify categorical columns (excluding 'Diabetes_binary' and numerical columns)
numerical_cols = ['BMI', 'MentHlth', 'PhysHlth']
categorical_features = [col for col in df.columns if col not in numerical_cols and col != 'Diabetes_binary']

features = categorical_features
n_features = len(features)
n_cols = 3  # Adjust the number of columns for displaying plots
n_rows = (n_features + n_cols - 1) // n_cols

plt.figure(figsize=(15, n_rows * 5))

# Create a copy to avoid SettingWithCopyWarning when adding binned column
df_subset_plot = df.copy()

# Convert 'Diabetes_binary' to numeric
df_subset_plot['Diabetes_binary'] = pd.to_numeric(df_subset_plot['Diabetes_binary'])

for i, col in enumerate(features):
    plt.subplot(n_rows, n_cols, i + 1)
    # Check if the column needs to be treated as categorical for plotting
    if col in ['GenHlth', 'Age', 'Income']:
        # Convert to appropriate type and sort unique values
        if col == 'Age':
            df_subset_plot[col] = pd.to_numeric(df_subset_plot[col])
            order = sorted(df_subset_plot[col].unique())
        else:
            df_subset_plot[col] = df_subset_plot[col].astype(str)
            order = sorted(df_subset_plot[col].unique())

        sns.barplot(data=df_subset_plot, x=col, y='Diabetes_binary', errorbar=None, order=order)
        plt.title(f'Percentage with Diabetes by {col}')
        plt.xlabel(col)
        plt.ylabel('Percentage with Diabetes')
        plt.xticks(rotation=45, ha='right')
    elif df_subset_plot[col].dtype in ['float64', 'int64'] and len(df_subset_plot[col].unique()) > 10: # Adjusted threshold for binning
        # Convert to numeric first for binning
        df_subset_plot[f'{col}_numeric'] = pd.to_numeric(df_subset_plot[col])
        # Bin the numerical data and calculate the mean of Diabetes_binary for each bin (percentage with diabetes)
        df_subset_plot[f'{col}_binned'] = pd.cut(df_subset_plot[f'{col}_numeric'], bins=10, duplicates='drop') # Adjust number of bins as needed
        sns.barplot(data=df_subset_plot, x=f'{col}_binned', y='Diabetes_binary', errorbar=None)
        plt.title(f'Percentage with Diabetes by {col} Bins')
        plt.xlabel(col)
        plt.ylabel('Percentage with Diabetes')
        plt.xticks(rotation=45, ha='right')
    else:
        # For other categorical/binary variables, calculate the mean of Diabetes_binary for each category
        # Ensure the column is treated as categorical for plotting
        df_subset_plot[col] = df_subset_plot[col].astype(str)
        order = sorted(df_subset_plot[col].unique())
        sns.barplot(data=df_subset_plot, x=col, y='Diabetes_binary', errorbar=None, order=order)
        plt.title(f'Percentage with Diabetes by {col}')
        plt.xlabel(col)
        plt.ylabel('Percentage with Diabetes')

plt.tight_layout()
plt.show()

# Clean up the binned columns from the copied DataFrame
for col in features:
    if f'{col}_binned' in df_subset_plot.columns:
        df_subset_plot = df_subset_plot.drop(columns=[f'{col}_binned', f'{col}_numeric'])

"""We observe that all categorical variables exhibit a noticeable pattern or relationship with the target variable (Diabetes); therefore, we will retain all of them at this stage of the analysis."""

# Distribution of continuous variables
# Box Plots
plt.figure(figsize=(12, 16)) # Adjusted figure size to accommodate more plots

# Box Plot for BMI
plt.subplot(4, 2, 1) # Create a 4x2 grid of plots and select the 1st subplot
sns.boxplot(data=df, x='Diabetes_binary', y='BMI')
plt.title('BMI by Diabetes Indicator')
plt.xlabel('Diabetes Indicator (0: No, 1: Yes)')
plt.ylabel('BMI')

# Box Plot for MentHlth
plt.subplot(4, 2, 3) # Select the 3rd subplot
sns.boxplot(data=df, x='Diabetes_binary', y='MentHlth')
plt.title('MentHlth by Diabetes Indicator', fontsize=14) # Increase title font size
plt.xlabel('Diabetes Indicator', fontsize=12) # Increase xlabel font size
plt.ylabel('MentHlth', fontsize=12) # Keep ylabel for the first plot

# Box Plot for PhysHlth
plt.subplot(4, 2, 4) # Select the 4th subplot
sns.boxplot(data=df,x='Diabetes_binary', y='PhysHlth')
plt.title('PhysHealth by Diabetes Indicator', fontsize=14) # Increase title font size
plt.xlabel('Diabetes Indicator', fontsize=12) # Increase xlabel font size
plt.ylabel('PhysHlth', fontsize=12) # Leave ylabel empty for the second plot

plt.tight_layout()
plt.show()

# Correlations
# Select only the continuous variables and the target variable
continuous_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cols_for_correlation = ['Diabetes_binary'] + continuous_cols

# Create a copy for correlation calculation and convert target to numeric
df_corr = df[cols_for_correlation].copy()
df_corr['Diabetes_binary'] = pd.to_numeric(df_corr['Diabetes_binary'])


# Create the correlation table
correlation_table = df_corr.corr()

# Display the correlation table with color
print("Correlation Table of Continuous Variables and Target:")
display(correlation_table.style.background_gradient(cmap='coolwarm', axis=None))

"""The box plots and correlation analysis of all continuous variables do not indicate a strong relationship with the target variable (Diabetes_binary); however, we will retain them at this stage for further analysis."""

# Check for missing values
missing_values = df.isnull().sum()

# Display the number of missing values per column
print("Missing values per column:")
display(missing_values)

"""There is no missing value in the dataset."""

# Check for outliers using a modified IQR method for numerical columns

numerical_cols = ['BMI', 'MentHlth', 'PhysHlth']

print("Outlier detection using modified IQR:")
for col in numerical_cols:
    # calculate interquartile range (IQR) using 10th and 90th percentiles
    q1 = df[col].quantile(0.1)
    q3 = df[col].quantile(0.9)
    iqr = q3 - q1

    # define the boundaries for an outlier with a multiplier of 3
    lower_bound = q1 - (3 * iqr)
    upper_bound = q3 + (3 * iqr)

    # find the outliers
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    # display the outliers
    print(f"\nColumn: {col}")
    print(f"  Number of outliers: {len(outliers)}")
    display(outliers[[col]].head()) # Display first few outlier values

"""There are 347 outliers in the BMI columns, we are going to drop all of them."""

# Drop outliers in BMI columns
numerical_cols = ['BMI'] # Only BMI had outliers based on the previous analysis

for col in numerical_cols:
    # calculate interquartile range (IQR) using 10th and 90th percentiles (same as in outlier detection)
    q1 = df[col].quantile(0.1)
    q3 = df[col].quantile(0.9)
    iqr = q3 - q1

    # define the boundaries for an outlier with a multiplier of 3
    lower_bound = q1 - (3 * iqr)
    upper_bound = q3 + (3 * iqr)

    # Drop the outliers
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)].copy() # Use .copy() to avoid SettingWithCopyWarning

print(f"DataFrame shape after dropping BMI outliers: {df.shape}")

# Check for outliers using Mahalanobis distance for numerical columns

from scipy.spatial import distance
from sklearn.preprocessing import StandardScaler
import numpy as np

# Select only the continuous numerical columns
numerical_cols_maha = ['BMI', 'MentHlth', 'PhysHlth']
data_for_maha = df[numerical_cols_maha].copy()

# Standardize the numerical data
scaler_maha = StandardScaler()
data_scaled_maha = scaler_maha.fit_transform(data_for_maha)

# Calculate the covariance matrix of the scaled data
covariance_matrix = np.cov(data_scaled_maha.T)

# Calculate the inverse of the covariance matrix
# Add a small value to the diagonal for numerical stability if needed
try:
    inv_cov = np.linalg.inv(covariance_matrix)
except np.linalg.LinAlgError:
    print("Warning: Singular covariance matrix. Adding a small value to the diagonal for inversion.")
    covariance_matrix += np.eye(covariance_matrix.shape[0]) * 1e-6
    inv_cov = np.linalg.inv(covariance_matrix)


# Calculate Mahalanobis distance for each data point
distances = []
for i in range(data_scaled_maha.shape[0]):
    row = data_scaled_maha[i, :]
    mahalanobis_distance = distance.mahalanobis(row, np.mean(data_scaled_maha, axis=0), inv_cov)
    distances.append(mahalanobis_distance)

# Set a threshold for outlier detection (e.g., based on chi-squared distribution or a chosen percentile)
# Using a common threshold for multivariate outliers based on chi-squared distribution with df = number of features
from scipy.stats import chi2
threshold = chi2.ppf(0.99, data_scaled_maha.shape[1]) # 0.99 confidence level, df = number of features

# Identify outliers
outliers_maha_indices = [i for i, d in enumerate(distances) if d > threshold]

print(f"Number of potential outliers based on Mahalanobis distance: {len(outliers_maha_indices)}")

# Optional: Display some of the outlier data points
# print("\nPotential Outlier Data Points (first 5):")
# display(df.iloc[outliers_maha_indices].head())


# Plot Mahalanobis distances
plt.figure(figsize=(10, 6))
# Use df.index as the x-axis since distances correspond to the original DataFrame rows
plt.plot(df.index, distances, marker='o', linestyle='', markersize=3)
plt.axhline(y=threshold, color='r', linestyle='-', label=f'Outlier Threshold ({threshold:.2f})')
plt.title('Mahalanobis Distance by Row Number')
plt.xlabel('Row Number')
plt.ylabel('Mahalanobis Distance')
plt.legend()
plt.grid(True)
plt.show()

"""Based on the Mahalanobis distance, no outliers were detected in the dataset.

# Partitioning
"""

from sklearn.model_selection import train_test_split

# Define the target variable
target = 'Diabetes_binary'
# Define features by dropping the target variable, it's label version, and the 'CustomerID' column
features = df.drop([target], axis=1)
target_variable = df[target]

# Split the data into 80% training and 20% test
X_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)

# Split the 80% training data into 60% training and 20% validation
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2

# Print the shapes of the resulting datasets
print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)
print("Test set shape:", X_test.shape, y_test.shape)

# Distribution of Training/Val/Test sets
import matplotlib.pyplot as plt
import seaborn as sns

# Combine the features and target for plotting
train_data = X_train.copy()
train_data['Diabetes_binary'] = y_train

val_data = X_val.copy()
val_data['Diabetes_binary'] = y_val

test_data = X_test.copy()
test_data['Diabetes_binary'] = y_test

# Plot the distribution of 'Diabetes_binary' in each partition
fig, axs = plt.subplots(1, 3, figsize=(18, 6))

sns.countplot(data=train_data, x='Diabetes_binary', ax=axs[0])
axs[0].set_title('Training Set - Diabetes_binary Distribution')
axs[0].set_xlabel('Diabetes_binary')
axs[0].set_ylabel('Count')
axs[0].set_xticks([0, 1])
axs[0].set_xticklabels(['No', 'Yes'])
for p in axs[0].patches:
    axs[0].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

sns.countplot(data=val_data, x='Diabetes_binary', ax=axs[1])
axs[1].set_title('Validation Set - Diabetes_binary Distribution')
axs[1].set_xlabel('Diabetes_binary')
axs[1].set_ylabel('Count')
axs[1].set_xticks([0, 1])
axs[1].set_xticklabels(['No', 'Yes'])
for p in axs[1].patches:
    axs[1].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')


sns.countplot(data=test_data, x='Diabetes_binary', ax=axs[2])
axs[2].set_title('Test Set - Diabetes_binary Distribution')
axs[2].set_xlabel('Diabetes_binary')
axs[2].set_ylabel('Count')
axs[2].set_xticks([0, 1])
axs[2].set_xticklabels(['No', 'Yes'])
for p in axs[2].patches:
    axs[2].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.tight_layout()
plt.show()

# Print counts and percentages for each partition
print("Training set Diabetes_binary counts and percentages:")
train_counts = train_data['Diabetes_binary'].value_counts()
train_percentages = train_data['Diabetes_binary'].value_counts(normalize=True) * 100
print(train_counts)
print(train_percentages.round(2)) # Round percentages to 2 decimal places


print("\nValidation set Diabetes_binary counts and percentages:")
val_counts = val_data['Diabetes_binary'].value_counts()
val_percentages = val_data['Diabetes_binary'].value_counts(normalize=True) * 100
print(val_counts)
print(val_percentages.round(2)) # Round percentages to 2 decimal bases


print("\nTest set Diabetes_binary counts and percentages:")
test_counts = test_data['Diabetes_binary'].value_counts()
test_percentages = test_data['Diabetes_binary'].value_counts(normalize=True) * 100
print(test_counts)
print(test_percentages.round(2)) # Round percentages to 2 decimal places

"""# Logistic Regression Model

## Iteration 1 - All Features
"""

# Initialize and train the Logistic Regression model
logistic_model = LogisticRegression(random_state=42)
logistic_model.fit(X_train, y_train)

print("Logistic Regression model trained successfully.")

# Make predictions (classes) on the training, validation, and test sets
y_train_pred = logistic_model.predict(X_train)
y_val_pred = logistic_model.predict(X_val)
y_test_pred = logistic_model.predict(X_test)

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = logistic_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = logistic_model.predict_proba(X_val)[:, 1]
y_test_proba = logistic_model.predict_proba(X_test)[:, 1]

print("Predictions (classes and probabilities) made successfully on training, validation, and test sets.")

"""###Model Assessment Iteration 1

"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate confusion matrix for the training set
cm_train = confusion_matrix(y_train, y_train_pred)
print("Confusion Matrix - Training Set:")
print(cm_train)

# Calculate confusion matrix for the validation set
cm_val = confusion_matrix(y_val, y_val_pred)
print("\nConfusion Matrix - Validation Set:")
print(cm_val)

# Calculate confusion matrix for the test set
cm_test = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix - Test Set:")
print(cm_test)

# Optional: Visualize Confusion Matrices
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0], annot_kws={"size": 12})
axes[0].set_title('Confusion Matrix - Training Set')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[1], annot_kws={"size": 12})
axes[1].set_title('Confusion Matrix - Validation Set')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[2], annot_kws={"size": 12})
axes[2].set_title('Confusion Matrix - Test Set')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Show the coefficients of the logistic regression model, including the intercept
feature_names = X_train.columns
coefficients = logistic_model.coef_[0]
intercept = logistic_model.intercept_[0]

# Create a DataFrame for coefficients and feature names
coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})

# Add the intercept to the DataFrame
intercept_df = pd.DataFrame({'Feature': ['Intercept'], 'Coefficient': [intercept]})
coef_df = pd.concat([intercept_df, coef_df], ignore_index=True)

print("\nCoefficients (including Intercept) with Feature Names:")
print(coef_df)

# Show the regression formula
# The formula for logistic regression is: log(p / (1 - p)) = b0 + b1*x1 + b2*x2 + ... + bn*xn
# Rearranging for p: p = 1 / (1 + exp(-(b0 + b1*x1 + b2*x2 + ... + bn*xn)))

intercept = logistic_model.intercept_[0]

linear_combination = f"{intercept:.4f}"
for i, feature in enumerate(feature_names):
    linear_combination += f" + {coefficients[i]:.4f}*{feature}"

formula = f"p = 1 / (1 + exp(-({linear_combination})))"

print("Logistic Regression Formula (with probability on the left):")
print(formula)

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred))

# Calculate ROC curve and AUC for the training set
# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

# Convert relevant columns to numeric for statsmodels
X_train_sm = X_train.copy()
for col in X_train_sm.columns:
    # Attempt to convert columns to numeric, coercing errors to NaN
    X_train_sm[col] = pd.to_numeric(X_train_sm[col], errors='coerce')

# Drop any columns that could not be converted to numeric (if any)
X_train_sm = X_train_sm.dropna(axis=1)

# Add a constant (intercept) to the features for statsmodels
X_train_sm = sm.add_constant(X_train_sm)

# Fit the logistic regression model using statsmodels
# Convert y_train to numeric as well, as statsmodels expects numeric target
y_train_numeric_sm = pd.to_numeric(y_train, errors='coerce')

# Drop rows where y_train became NaN due to coercion errors
X_train_sm = X_train_sm[y_train_numeric_sm.notna()]
y_train_numeric_sm = y_train_numeric_sm.dropna()

logit_model = sm.Logit(y_train_numeric_sm, X_train_sm)
result = logit_model.fit()

# Show the model summary, which includes p-values
print(result.summary())

"""From the p-value table, we identified variables with p-values < 0.05 and used them to tune the model to reduce noise and improve stability.

##Iteration 2 - Tuning (selected features)
"""

# statistically significant features
selected_features = ['HighBP', 'HighChol','CholCheck','BMI','Stroke','HeartDiseaseorAttack','PhysActivity','Education',
                     'Veggies','HvyAlcoholConsump','NoDocbcCost','GenHlth','MentHlth','PhysHlth','DiffWalk','Sex','Age','Income']

# Use the updated train/validation/test splits after outlier removal
X_train_selected = X_train[selected_features]
X_val_selected = X_val[selected_features]
X_test_selected = X_test[selected_features]

# Initialize and train the Logistic Regression model with selected features
model_selected = LogisticRegression(random_state=42)
model_selected.fit(X_train_selected, y_train)

print("Logistic Regression model trained successfully.")

# Make predictions on the training, validation, and test sets with selected features
y_train_pred_selected = model_selected.predict(X_train_selected)
y_val_pred_selected = model_selected.predict(X_val_selected)
y_test_pred_selected = model_selected.predict(X_test_selected)

# Get predicted probabilities for the training, validation, and test sets with selected features
y_train_proba_selected = model_selected.predict_proba(X_train_selected)[:, 1] # Probability of the positive class (1)
y_val_proba_selected = model_selected.predict_proba(X_val_selected)[:, 1]
y_test_proba_selected = model_selected.predict_proba(X_test_selected)[:, 1]

print("Predictions (classes and probabilities) made successfully on training, validation, and test sets with selected features.")

"""###Model Assessment Iteration 2"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate confusion matrix for the training set
cm_train = confusion_matrix(y_train, y_train_pred_selected)
print("Confusion Matrix - Training Set:")
print(cm_train)

# Calculate confusion matrix for the validation set
cm_val = confusion_matrix(y_val, y_val_pred_selected)
print("\nConfusion Matrix - Validation Set:")
print(cm_val)

# Calculate confusion matrix for the test set
cm_test = confusion_matrix(y_test, y_test_pred_selected)
print("\nConfusion Matrix - Test Set:")
print(cm_test)

# Optional: Visualize Confusion Matrices
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0], annot_kws={"size": 12})
axes[0].set_title('Confusion Matrix - Training Set')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[1], annot_kws={"size": 12})
axes[1].set_title('Confusion Matrix - Validation Set')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[2], annot_kws={"size": 12})
axes[2].set_title('Confusion Matrix - Test Set')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_selected))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_selected))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_selected))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set with selected features
fpr_train_selected, tpr_train_selected, thresholds_train_selected = roc_curve(y_train_numeric, y_train_proba_selected)
roc_auc_train_selected = auc(fpr_train_selected, tpr_train_selected)

# Calculate ROC curve and AUC for the validation set with selected features
fpr_val_selected, tpr_val_selected, thresholds_val_selected = roc_curve(y_val_numeric, y_val_proba_selected)
roc_auc_val_selected = auc(fpr_val_selected, tpr_val_selected)

# Calculate ROC curve and AUC for the test set with selected features
fpr_test_selected, tpr_test_selected, thresholds_test_selected = roc_curve(y_test_numeric, y_test_proba_selected)
roc_auc_test_selected = auc(fpr_test_selected, tpr_test_selected)

# Plot ROC curves with selected features
plt.figure(figsize=(10, 8))
plt.plot(fpr_train_selected, tpr_train_selected, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train_selected)
plt.plot(fpr_val_selected, tpr_val_selected, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val_selected)
plt.plot(fpr_test_selected, tpr_test_selected, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test_selected)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (with Selected Features)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC (with selected features): {roc_auc_train_selected:.4f}")
print(f"Validation AUC (with selected features): {roc_auc_val_selected:.4f}")
print(f"Test AUC (with selected features): {roc_auc_test_selected:.4f}")

# Convert relevant columns to numeric for statsmodels
X_train_selected_sm = X_train_selected.copy()
for col in X_train_selected_sm.columns:
    # Attempt to convert columns to numeric, coercing errors to NaN
    X_train_selected_sm[col] = pd.to_numeric(X_train_selected_sm[col], errors='coerce')

# Drop any columns that could not be converted to numeric (if any)
X_train_selected_sm = X_train_selected_sm.dropna(axis=1)

# Add a constant (intercept) to the features for statsmodels
X_train_selected_sm = sm.add_constant(X_train_selected_sm)

# Fit the logistic regression model using statsmodels
# Convert y_train to numeric as well, as statsmodels expects numeric target
y_train_numeric_sm = pd.to_numeric(y_train, errors='coerce')

# Drop rows where y_train became NaN due to coercion errors
X_train_selected_sm = X_train_selected_sm[y_train_numeric_sm.notna()]
y_train_numeric_sm = y_train_numeric_sm.dropna()


logit_model_selected = sm.Logit(y_train_numeric_sm, X_train_selected_sm)
result_selected = logit_model_selected.fit()

# Show the model summary, which includes p-values
print(result_selected.summary())

"""We introduced an interaction term to capture potential combined effects between DiffWalk and Age and to test whether it could improve the modelâ€™s performance.

##Iteration 3 - Tuning (interaction features)
"""

# Create the interaction term between DiffWalk and Age off of the statistically significant columns df
X_train_interact = X_train_selected.copy()
X_val_interact = X_val_selected.copy()
X_test_interact = X_test_selected.copy()

# Convert 'DiffWalk' and 'Age' to numeric before creating the interaction term
X_train_interact['DiffWalk'] = pd.to_numeric(X_train_interact['DiffWalk'])
X_train_interact['Age'] = pd.to_numeric(X_train_interact['Age'])
X_val_interact['DiffWalk'] = pd.to_numeric(X_val_interact['DiffWalk'])
X_val_interact['Age'] = pd.to_numeric(X_val_interact['Age'])
X_test_interact['DiffWalk'] = pd.to_numeric(X_test_interact['DiffWalk'])
X_test_interact['Age'] = pd.to_numeric(X_test_interact['Age'])

X_train_interact['DiffWalk_Age_Interaction'] = X_train_interact['DiffWalk'] * X_train_interact['Age']
X_val_interact['DiffWalk_Age_Interaction'] = X_val_interact['DiffWalk'] * X_val_interact['Age']
X_test_interact['DiffWalk_Age_Interaction'] = X_test_interact['DiffWalk'] * X_test_interact['Age']

# Initialize and train the Logistic Regression model with the interaction term
logistic_model_interact = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter to address potential convergence issues
logistic_model_interact.fit(X_train_interact, y_train)

print("Logistic Regression model with DiffWalk and Age interaction term trained successfully.")

# Make predictions with the new model
y_train_pred_interact = logistic_model_interact.predict(X_train_interact)
y_val_pred_interact = logistic_model_interact.predict(X_val_interact)
y_test_pred_interact = logistic_model_interact.predict(X_test_interact)

# Get predicted probabilities with the new model
y_train_proba_interact = logistic_model_interact.predict_proba(X_train_interact)[:, 1]
y_val_proba_interact = logistic_model_interact.predict_proba(X_val_interact)[:, 1]
y_test_proba_interact = logistic_model_interact.predict_proba(X_test_interact)[:, 1]

print("Predictions with interaction term made successfully.")

"""Difficulty in walking or climbing up stairs may mean extremely different things depending on the age buckets. If someone is very old, even if they are healthy they may have a hard time walking up stairs, while if someone is young and has a hard time walking up stairs there are very different implications.

### Model Assessment Iteration 3
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate confusion matrix for the training set
cm_train = confusion_matrix(y_train, y_train_pred_interact)
print("Confusion Matrix - Training Set:")
print(cm_train)

# Calculate confusion matrix for the validation set
cm_val = confusion_matrix(y_val, y_val_pred_interact)
print("\nConfusion Matrix - Validation Set:")
print(cm_val)

# Calculate confusion matrix for the test set
cm_test = confusion_matrix(y_test, y_test_pred_interact)
print("\nConfusion Matrix - Test Set:")
print(cm_test)

# Optional: Visualize Confusion Matrices
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0], annot_kws={"size": 12})
axes[0].set_title('Confusion Matrix - Training Set')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[1], annot_kws={"size": 12})
axes[1].set_title('Confusion Matrix - Validation Set')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[2], annot_kws={"size": 12})
axes[2].set_title('Confusion Matrix - Test Set')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')

plt.tight_layout()
plt.show()

# Show classification report for the training set with interaction term
print("Classification Report - Training Set (with Interaction Term):")
print(classification_report(y_train, y_train_pred_interact))

# Show classification report for the validation set with interaction term
print("\nClassification Report - Validation Set (with Interaction Term):")
print(classification_report(y_val, y_val_pred_interact))

# Show classification report for the test set with interaction term
print("\nClassification Report - Test Set (with Interaction Term):")
print(classification_report(y_test, y_test_pred_interact))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set with interaction term
fpr_train_interact, tpr_train_interact, thresholds_train_interact = roc_curve(y_train_numeric, y_train_proba_interact)
roc_auc_train_interact = auc(fpr_train_interact, tpr_train_interact)

# Calculate ROC curve and AUC for the validation set with interaction term
fpr_val_interact, tpr_val_interact, thresholds_val_interact = roc_curve(y_val_numeric, y_val_proba_interact)
roc_auc_val_interact = auc(fpr_val_interact, tpr_val_interact)

# Calculate ROC curve and AUC for the test set with interaction term
fpr_test_interact, tpr_test_interact, thresholds_test_interact = roc_curve(y_test_numeric, y_test_proba_interact)
roc_auc_test_interact = auc(fpr_test_interact, tpr_test_interact)

# Plot ROC curves with interaction term
plt.figure(figsize=(10, 8))
plt.plot(fpr_train_interact, tpr_train_interact, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train_interact)
plt.plot(fpr_val_interact, tpr_val_interact, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val_interact)
plt.plot(fpr_test_interact, tpr_test_interact, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test_interact)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (with Interaction Term)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC (with Interaction Term): {roc_auc_train_interact:.4f}")
print(f"Validation AUC (with Interaction Term): {roc_auc_val_interact:.4f}")
print(f"Test AUC (with Interaction Term): {roc_auc_test_interact:.4f}")

# Convert relevant columns to numeric for statsmodels
X_train_interact_sm = X_train_interact.copy()
for col in X_train_interact_sm.columns:
    # Attempt to convert columns to numeric, coercing errors to NaN
    X_train_interact_sm[col] = pd.to_numeric(X_train_interact_sm[col], errors='coerce')

# Drop any columns that could not be converted to numeric (if any)
X_train_interact_sm = X_train_interact_sm.dropna(axis=1)

# Add a constant (intercept) to the features for statsmodels
X_train_interact_sm = sm.add_constant(X_train_interact_sm)

# Fit the logistic regression model using statsmodels
# Convert y_train to numeric as well, as statsmodels expects numeric target
y_train_numeric_sm = pd.to_numeric(y_train, errors='coerce')

# Drop rows where y_train became NaN due to coercion errors
X_train_interact_sm = X_train_interact_sm[y_train_numeric_sm.notna()]
y_train_numeric_sm = y_train_numeric_sm.dropna()


logit_model_interact = sm.Logit(y_train_numeric_sm, X_train_interact_sm)
result_interact = logit_model_interact.fit()

# Show the model summary, which includes p-values
print(result_interact.summary())

"""Since the p-value for the 'NoDocbcCost' variable is 0.130, which is greater than the significance level of 0.05, this variable is not statistically significant and will be removed from the dataset.

We used oversampling on the model with selected features to correct the class imbalance and improve prediction of the minority class.

##Iteration 4 - Oversampling and Selected Features
"""

# Start with the feature sets from Iteration 3, which include the interaction term
X_train_iter4 = X_train_interact.copy()
X_val_iter4 = X_val_interact.copy()
X_test_iter4 = X_test_interact.copy()

# Define the feature to remove for Iteration 4
feature_to_remove_iter4 = 'NoDocbcCost'

# Remove 'NoDocbcCost' if it exists in the feature sets
if feature_to_remove_iter4 in X_train_iter4.columns:
    X_train_iter4 = X_train_iter4.drop(columns=[feature_to_remove_iter4])
if feature_to_remove_iter4 in X_val_iter4.columns:
    X_val_iter4 = X_val_iter4.drop(columns=[feature_to_remove_iter4])
if feature_to_remove_iter4 in X_test_iter4.columns:
    X_test_iter4 = X_test_iter4.drop(columns=[feature_to_remove_iter4])

# Update the list of selected features for this iteration based on the columns actually present
selected_features_iter4 = X_train_iter4.columns.tolist()

print("New feature sets created successfully for Iteration 4.")
print(f"Features for Iteration 4: {selected_features_iter4}")
print(f"X_train_iter4 shape: {X_train_iter4.shape}")
print(f"X_val_iter4 shape: {X_val_iter4.shape}")
print(f"X_test_iter4 shape: {X_test_iter4.shape}")

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_iter4, y_train)

# Print the shape of the original and oversampled target variables
print("Original y_train shape:", y_train.shape)
print("Oversampled y_train_oversampled shape:", y_train_oversampled.shape)
print("Class distribution after SMOTE:\n", y_train_oversampled.value_counts())

logistic_model_resampled = LogisticRegression(random_state=42, max_iter=1000)
logistic_model_resampled.fit(X_train_oversampled, y_train_oversampled)

print("Logistic Regression model trained successfully with oversampled data.")

"""###Model Assessment Iteration 4"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Make predictions (classes) on the oversampled training, validation, and test sets
y_train_pred_oversampled = logistic_model_resampled.predict(X_train_oversampled)
y_val_pred_oversampled = logistic_model_resampled.predict(X_val_iter4)
y_test_pred_oversampled = logistic_model_resampled.predict(X_test_iter4)

# Convert predictions to numeric type for consistent comparison with y_true
y_train_pred_oversampled_numeric = pd.to_numeric(y_train_pred_oversampled)
y_val_pred_oversampled_numeric = pd.to_numeric(y_val_pred_oversampled)
y_test_pred_oversampled_numeric = pd.to_numeric(y_test_pred_oversampled)

# Get predicted probabilities for the oversampled training, validation, and test sets
y_train_proba_oversampled = logistic_model_resampled.predict_proba(X_train_oversampled)[:, 1]
y_val_proba_oversampled = logistic_model_resampled.predict_proba(X_val_iter4)[:, 1]
y_test_proba_oversampled = logistic_model_resampled.predict_proba(X_test_iter4)[:, 1]

print("Predictions (classes and probabilities) made successfully on oversampled training, validation, and test sets.")

# Confusion Matrices
# Convert y_train_oversampled to numeric for confusion_matrix
y_train_oversampled_numeric = pd.to_numeric(y_train_oversampled)

cm_train_oversampled = confusion_matrix(y_train_oversampled_numeric, y_train_pred_oversampled_numeric)
print("\nConfusion Matrix - Oversampled Training Set:")
print(cm_train_oversampled)

cm_val_oversampled = confusion_matrix(y_val_numeric, y_val_pred_oversampled_numeric)
print("\nConfusion Matrix - Validation Set:")
print(cm_val_oversampled)

cm_test_oversampled = confusion_matrix(y_test_numeric, y_test_pred_oversampled_numeric)
print("\nConfusion Matrix - Test Set:")
print(cm_test_oversampled)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
sns.heatmap(cm_train_oversampled, annot=True, fmt='d', cmap='Blues', ax=axes[0], annot_kws={"size": 12})
axes[0].set_title('Confusion Matrix - Oversampled Training Set')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(cm_val_oversampled, annot=True, fmt='d', cmap='Blues', ax=axes[1], annot_kws={"size": 12})
axes[1].set_title('Confusion Matrix - Validation Set')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

sns.heatmap(cm_test_oversampled, annot=True, fmt='d', cmap='Blues', ax=axes[2], annot_kws={"size": 12})
axes[2].set_title('Confusion Matrix - Test Set')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
plt.tight_layout()
plt.show()

# Classification Reports
print("\nClassification Report - Oversampled Training Set:")
print(classification_report(y_train_oversampled_numeric, y_train_pred_oversampled_numeric))

print("\nClassification Report - Validation Set:")
print(classification_report(y_val_numeric, y_val_pred_oversampled_numeric))

print("\nClassification Report - Test Set:")
print(classification_report(y_test_numeric, y_test_pred_oversampled_numeric))

# ROC Curves and AUC Scores
fpr_train_oversampled, tpr_train_oversampled, _ = roc_curve(y_train_oversampled_numeric, y_train_proba_oversampled)
roc_auc_train_oversampled = auc(fpr_train_oversampled, tpr_train_oversampled)

fpr_val_oversampled, tpr_val_oversampled, _ = roc_curve(y_val_numeric, y_val_proba_oversampled)
roc_auc_val_oversampled = auc(fpr_val_oversampled, tpr_val_oversampled)

fpr_test_oversampled, tpr_test_oversampled, _ = roc_curve(y_test_numeric, y_test_proba_oversampled)
roc_auc_test_oversampled = auc(fpr_test_oversampled, tpr_test_oversampled)

plt.figure(figsize=(10, 8))
plt.plot(fpr_train_oversampled, tpr_train_oversampled, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train_oversampled)
plt.plot(fpr_val_oversampled, tpr_val_oversampled, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val_oversampled)
plt.plot(fpr_test_oversampled, tpr_test_oversampled, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test_oversampled)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Oversampled Model)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC (Oversampled Model): {roc_auc_train_oversampled:.4f}")
print(f"Validation AUC (Oversampled Model): {roc_auc_val_oversampled:.4f}")
print(f"Test AUC (Oversampled Model): {roc_auc_test_oversampled:.4f}")

import statsmodels.api as sm
import pandas as pd

# Convert relevant columns to numeric for statsmodels
X_train_oversampled_sm = X_train_oversampled.copy()
for col in X_train_oversampled_sm.columns:
    X_train_oversampled_sm[col] = pd.to_numeric(X_train_oversampled_sm[col], errors='coerce')
X_train_oversampled_sm = X_train_oversampled_sm.dropna(axis=1)
X_train_oversampled_sm = sm.add_constant(X_train_oversampled_sm)
y_train_oversampled_numeric_sm = pd.to_numeric(y_train_oversampled, errors='coerce')
X_train_oversampled_sm = X_train_oversampled_sm[y_train_oversampled_numeric_sm.notna()]
y_train_oversampled_numeric_sm = y_train_oversampled_numeric_sm.dropna()

logit_model_oversampled = sm.Logit(y_train_oversampled_numeric_sm, X_train_oversampled_sm)
result_oversampled = logit_model_oversampled.fit()

print("\nStatsmodels Summary (Oversampled Model):")
print(result_oversampled.summary())

"""All key variables shown are statistically significant.

Reviewing the Model Performance Metrics of all interations, the Oversampled Logistic Regression Model exibited the highest overall performance. Nonetheless, we noted a trade-off: precision decreased moderately while recall improved significantly.

# Decision Tree Model

## Iteration 1 - All Features
"""

from sklearn.tree import DecisionTreeClassifier

# Initialize the Decision Tree Classifier
# We can start with default parameters or set some like max_depth for initial exploration
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model on the training data
dt_model.fit(X_train, y_train)

print("Decision Tree model trained successfully.")

# The number of splits in a decision tree is the number of internal nodes.
# This is equal to the total number of nodes minus the number of leaf nodes.
# We can access the tree structure from the tuned_dt_model.tree_ attribute.

tree_ = dt_model.tree_
total_nodes = tree_.node_count
leaf_nodes = tree_.n_leaves

number_of_splits = total_nodes - leaf_nodes

print(f"The decision tree has {number_of_splits} splits (internal nodes).")
print(f"The decision tree has {leaf_nodes} leaves.")

"""### Model Assessment Iteration 1"""

from sklearn.metrics import confusion_matrix

# Predictions on training set
y_train_pred_dt = dt_model.predict(X_train)
cm_train = confusion_matrix(y_train, y_train_pred_dt)

# Predictions on validation set
y_val_pred_dt = dt_model.predict(X_val)
cm_val = confusion_matrix(y_val, y_val_pred_dt)

# Predictions on test set
y_test_pred_dt = dt_model.predict(X_test)
cm_test = confusion_matrix(y_test, y_test_pred_dt)

print("Confusion Matrix - Training Set:")
print(cm_train)

print("\nConfusion Matrix - Validation Set:")
print(cm_val)

print("\nConfusion Matrix - Test Set:")
print(cm_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_dt = pd.to_numeric(y_train_pred_dt)
y_val_pred_numeric_dt = pd.to_numeric(y_val_pred_dt)
y_test_pred_numeric_dt = pd.to_numeric(y_test_pred_dt)


# Calculate metrics for Training Set
accuracy_train = accuracy_score(y_train_numeric, y_train_pred_numeric_dt)
precision_train = precision_score(y_train_numeric, y_train_pred_numeric_dt)
recall_train = recall_score(y_train_numeric, y_train_pred_numeric_dt)
f1_train = f1_score(y_train_numeric, y_train_pred_numeric_dt)

print("Training Set Metrics:")
print(f"  Accuracy: {accuracy_train:.4f}")
print(f"  Precision: {precision_train:.4f}")
print(f"  Recall: {recall_train:.4f}")
print(f"  F1-Score: {f1_train:.4f}")

# Calculate metrics for Validation Set
accuracy_val = accuracy_score(y_val_numeric, y_val_pred_numeric_dt)
precision_val = precision_score(y_val_numeric, y_val_pred_numeric_dt)
recall_val = recall_score(y_val_numeric, y_val_pred_numeric_dt)
f1_val = f1_score(y_val_numeric, y_val_pred_numeric_dt)

print("\nValidation Set Metrics:")
print(f"  Accuracy: {accuracy_val:.4f}")
print(f"  Precision: {precision_val:.4f}")
print(f"  Recall: {recall_val:.4f}")
print(f"  F1-Score: {f1_val:.4f}")


# Calculate metrics for Test Set
accuracy_test = accuracy_score(y_test_numeric, y_test_pred_numeric_dt)
precision_test = precision_score(y_test_numeric, y_test_pred_numeric_dt)
recall_test = recall_score(y_test_numeric, y_test_pred_numeric_dt)
f1_test = f1_score(y_test_numeric, y_test_pred_numeric_dt)

print("\nTest Set Metrics:")
print(f"  Accuracy: {accuracy_test:.4f}")
print(f"  Precision: {precision_test:.4f}")
print(f"  Recall: {recall_test:.4f}")
print(f"  F1-Score: {f1_test:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_dt))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_dt))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_dt))

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = dt_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = dt_model.predict_proba(X_val)[:, 1]
y_test_proba = dt_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the decision tree model
feature_importances_dt = dt_model.feature_importances_

# Get the feature names from the training data (assuming X_train is consistent)
feature_names_dt = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_dt = pd.Series(feature_importances_dt, index=feature_names_dt)

# Sort the feature importances in descending order
sorted_feature_importances_dt = feature_importance_series_dt.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Decision Tree:")
print(sorted_feature_importances_dt)

"""As the model appears to be overfitted, we will implement additional tuning techniques to enhance its generalization and overall performance.

## Iteration 2 - Tuned Parameters
"""

# Define the parameter grid
param_grid = {
    'max_depth': [2,5,10],
    'min_samples_split': [2,5,10],
    'min_samples_leaf': [2,5,10]
}

print("Parameter grid defined:")
print(param_grid)

from sklearn.model_selection import GridSearchCV

# Instantiate GridSearchCV
grid_search = GridSearchCV(dt_model, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and best cross-validation score
print("Best hyperparameters found:", grid_search.best_params_)
print("Best cross-validation F1-score:", grid_search.best_score_)

# The number of splits in a decision tree is the number of internal nodes.
# This is equal to the total number of nodes minus the number of leaf nodes.
# We can access the tree structure from the tuned_dt_model.tree_ attribute.

# Retrieve the best hyperparameters
best_params = grid_search.best_params_
print("Best hyperparameters:", best_params)


# Instantiate a new Decision Tree Classifier with the best hyperparameters
tuned_dt_model = DecisionTreeClassifier(**best_params, random_state=42)

# Train the tuned model on the complete training dataset
tuned_dt_model.fit(X_train, y_train)

tree_ = tuned_dt_model.tree_
total_nodes = tree_.node_count
leaf_nodes = tree_.n_leaves

number_of_splits = total_nodes - leaf_nodes

print(f"The tuned decision tree has {number_of_splits} splits (internal nodes).")
print(f"The tuned decision tree has {leaf_nodes} leaves.")

"""### Model Assessment Iteration 2"""

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Make predictions on training set
y_train_pred_tuned = tuned_dt_model.predict(X_train)
cm_train_tuned = confusion_matrix(y_train, y_train_pred_tuned)

# Make predictions on validation set
y_val_pred_tuned = tuned_dt_model.predict(X_val)
cm_val_tuned = confusion_matrix(y_val, y_val_pred_tuned)

# Make predictions on test set
y_test_pred_tuned = tuned_dt_model.predict(X_test)
cm_test_tuned = confusion_matrix(y_test, y_test_pred_tuned)

print("\nConfusion Matrix - Tuned Training Set:")
print(cm_train_tuned)

print("\nConfusion Matrix - Tuned Validation Set:")
print(cm_val_tuned)

print("\nConfusion Matrix - Tuned Test Set:")
print(cm_test_tuned)

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_tuned = pd.to_numeric(y_train_pred_tuned)
y_val_pred_numeric_tuned = pd.to_numeric(y_val_pred_tuned)
y_test_pred_numeric_tuned = pd.to_numeric(y_test_pred_tuned)

# Calculate metrics for Training Set
accuracy_train_tuned = accuracy_score(y_train_numeric, y_train_pred_numeric_tuned)
precision_train_tuned = precision_score(y_train_numeric, y_train_pred_numeric_tuned)
recall_train_tuned = recall_score(y_train_numeric, y_train_pred_numeric_tuned)
f1_train_tuned = f1_score(y_train_numeric, y_train_pred_numeric_tuned)

print("\nTuned Training Set Metrics:")
print(f"  Accuracy: {accuracy_train_tuned:.4f}")
print(f"  Precision: {precision_train_tuned:.4f}")
print(f"  Recall: {recall_train_tuned:.4f}")
print(f"  F1-Score: {f1_train_tuned:.4f}")

# Calculate metrics for Validation Set
accuracy_val_tuned = accuracy_score(y_val_numeric, y_val_pred_numeric_tuned)
precision_val_tuned = precision_score(y_val_numeric, y_val_pred_numeric_tuned)
recall_val_tuned = recall_score(y_val_numeric, y_val_pred_numeric_tuned)
f1_val_tuned = f1_score(y_val_numeric, y_val_pred_numeric_tuned)

print("\nTuned Validation Set Metrics:")
print(f"  Accuracy: {accuracy_val_tuned:.4f}")
print(f"  Precision: {precision_val_tuned:.4f}")
print(f"  Recall: {recall_val_tuned:.4f}")
print(f"  F1-Score: {f1_val_tuned:.4f}")


# Calculate metrics for Test Set
accuracy_test_tuned = accuracy_score(y_test_numeric, y_test_pred_numeric_tuned)
precision_test_tuned = precision_score(y_test_numeric, y_test_pred_numeric_tuned)
recall_test_tuned = recall_score(y_test_numeric, y_test_pred_numeric_tuned)
f1_test_tuned = f1_score(y_test_numeric, y_test_pred_numeric_tuned)

print("\nTuned Test Set Metrics:")
print(f"  Accuracy: {accuracy_test_tuned:.4f}")
print(f"  Precision: {precision_test_tuned:.4f}")
print(f"  Recall: {recall_test_tuned:.4f}")
print(f"  F1-Score: {f1_test_tuned:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_tuned))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_tuned))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_tuned))

"""Although tuning helped mitigate overfitting, the performance metrics remained relatively weak, with recall staying low across all three sets."""

# Get predicted probabilities for the training, validation, and test sets
y_train_proba_tuned = tuned_dt_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba_tuned = tuned_dt_model.predict_proba(X_val)[:, 1]
y_test_proba_tuned = tuned_dt_model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba_tuned)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba_tuned)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba_tuned)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the tuned decision tree model
feature_importances_tuned_dt = tuned_dt_model.feature_importances_

# Get the feature names from the training data (assuming X_train is consistent)
feature_names_dt = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_tuned_dt = pd.Series(feature_importances_tuned_dt, index=feature_names_dt)

# Sort the feature importances in descending order
sorted_feature_importances_tuned_dt = feature_importance_series_tuned_dt.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Tuned Decision Tree:")
print(sorted_feature_importances_tuned_dt)

"""# Boosted Tree Model

## Iteration 1 - All Features
"""

from sklearn.ensemble import GradientBoostingClassifier

# Instantiate a Gradient Boosting Regressor model
gbc_model = GradientBoostingClassifier(random_state=42)

# Train the model on the training data
gbc_model.fit(X_train, y_train)

# Make predictions on training set
y_train_pred_gbc = gbc_model.predict(X_train)

# Make predictions on validation set
y_val_pred_gbc = gbc_model.predict(X_val)

# Make predictions on test set
y_test_pred_gbc = gbc_model.predict(X_test)

"""### Model Assessment Iteration 1"""

from sklearn.metrics import confusion_matrix

# Predictions on training set
cm_train = confusion_matrix(y_train, y_train_pred_gbc)

# Predictions on validation set
cm_val = confusion_matrix(y_val, y_val_pred_gbc)

# Predictions on test set
cm_test = confusion_matrix(y_test, y_test_pred_gbc)

print("Confusion Matrix - Training Set:")
print(cm_train)

print("\nConfusion Matrix - Validation Set:")
print(cm_val)

print("\nConfusion Matrix - Test Set:")
print(cm_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_gbc = pd.to_numeric(y_train_pred_gbc)
y_val_pred_numeric_gbc = pd.to_numeric(y_val_pred_gbc)
y_test_pred_numeric_gbc = pd.to_numeric(y_test_pred_gbc)


# Calculate metrics for Training Set
accuracy_train = accuracy_score(y_train_numeric, y_train_pred_numeric_gbc)
precision_train = precision_score(y_train_numeric, y_train_pred_numeric_gbc)
recall_train = recall_score(y_train_numeric, y_train_pred_numeric_gbc)
f1_train = f1_score(y_train_numeric, y_train_pred_numeric_gbc)

print("Training Set Metrics:")
print(f"  Accuracy: {accuracy_train:.4f}")
print(f"  Precision: {precision_train:.4f}")
print(f"  Recall: {recall_train:.4f}")
print(f"  F1-Score: {f1_train:.4f}")

# Calculate metrics for Validation Set
accuracy_val = accuracy_score(y_val_numeric, y_val_pred_numeric_gbc)
precision_val = precision_score(y_val_numeric, y_val_pred_numeric_gbc)
recall_val = recall_score(y_val_numeric, y_val_pred_numeric_gbc)
f1_val = f1_score(y_val_numeric, y_val_pred_numeric_gbc)

print("\nValidation Set Metrics:")
print(f"  Accuracy: {accuracy_val:.4f}")
print(f"  Precision: {precision_val:.4f}")
print(f"  Recall: {recall_val:.4f}")
print(f"  F1-Score: {f1_val:.4f}")


# Calculate metrics for Test Set
accuracy_test = accuracy_score(y_test_numeric, y_test_pred_numeric_gbc)
precision_test = precision_score(y_test_numeric, y_test_pred_numeric_gbc)
recall_test = recall_score(y_test_numeric, y_test_pred_numeric_gbc)
f1_test = f1_score(y_test_numeric, y_test_pred_numeric_gbc)

print("\nTest Set Metrics:")
print(f"  Accuracy: {accuracy_test:.4f}")
print(f"  Precision: {precision_test:.4f}")
print(f"  Recall: {recall_test:.4f}")
print(f"  F1-Score: {f1_test:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_gbc))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_gbc))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_gbc))

"""Model performs well on class 0 but shows consistently low recall for the minority class 1 across train, validation, and test sets."""

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = gbc_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = gbc_model.predict_proba(X_val)[:, 1]
y_test_proba = gbc_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the tuned gradient boosting regressor model
feature_importances_gbc = gbc_model.feature_importances_

# Get the feature names from the training data
feature_names = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_gbc = pd.Series(feature_importances_gbc, index=feature_names)

# Sort the feature importances in descending order
sorted_feature_importances_gbc = feature_importance_series_gbc.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Gradient Boosting Classifier:")
print(sorted_feature_importances_gbc)

"""## Iteration 2 - Tuned Parameters"""

param_grid_gbc = {
    'n_estimators': [100, 300],  # Number of boosting stages
    'learning_rate': [0.1], # Shrinkage on the contributions of each tree
    'max_depth': [3, 5],           # Maximum depth of the individual regression estimators
    'min_samples_split': [2, 10]    # Minimum number of samples required to split an internal node
}


# Instantiate GridSearchCV with the Gradient Boosting Classifier and the parameter grid
grid_search_gbc = GridSearchCV(gbc_model, param_grid_gbc, cv=2, scoring='f1_weighted', n_jobs=-1,verbose=3)


# Fit GridSearchCV to the training data
grid_search_gbc.fit(X_train, y_train)

# Retrieve the best hyperparameters
best_params = grid_search_gbc.best_params_
print("Best hyperparameters:", best_params)


# Instantiate a new GB Classifier with the best hyperparameters
tuned_gbc_model = GradientBoostingClassifier(**best_params, random_state=42)

# Train the tuned model on the complete training dataset
tuned_gbc_model.fit(X_train, y_train)

# Make predictions on training set
y_train_pred_tuned_gbc = tuned_gbc_model.predict(X_train)

# Make predictions on validation set
y_val_pred_tuned_gbc = tuned_gbc_model.predict(X_val)

# Make predictions on test set
y_test_pred_tuned_gbc = tuned_gbc_model.predict(X_test)

"""### Model Assessment Iteration 2"""

from sklearn.metrics import confusion_matrix

# Predictions on training set
cm_train = confusion_matrix(y_train, y_train_pred_tuned_gbc)

# Predictions on validation set
cm_val = confusion_matrix(y_val, y_val_pred_tuned_gbc)

# Predictions on test set
cm_test = confusion_matrix(y_test, y_test_pred_tuned_gbc)

print("Confusion Matrix - Training Set:")
print(cm_train)

print("\nConfusion Matrix - Validation Set:")
print(cm_val)

print("\nConfusion Matrix - Test Set:")
print(cm_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_tuned_gbc = pd.to_numeric(y_train_pred_tuned_gbc)
y_val_pred_numeric_tuned_gbc = pd.to_numeric(y_val_pred_tuned_gbc)
y_test_pred_numeric_tuned_gbc = pd.to_numeric(y_test_pred_tuned_gbc)


# Calculate metrics for Training Set
accuracy_train = accuracy_score(y_train_numeric, y_train_pred_numeric_tuned_gbc)
precision_train = precision_score(y_train_numeric, y_train_pred_numeric_tuned_gbc)
recall_train = recall_score(y_train_numeric, y_train_pred_numeric_tuned_gbc)
f1_train = f1_score(y_train_numeric, y_train_pred_numeric_tuned_gbc)

print("Training Set Metrics:")
print(f"  Accuracy: {accuracy_train:.4f}")
print(f"  Precision: {precision_train:.4f}")
print(f"  Recall: {recall_train:.4f}")
print(f"  F1-Score: {f1_train:.4f}")

# Calculate metrics for Validation Set
accuracy_val = accuracy_score(y_val_numeric, y_val_pred_numeric_tuned_gbc)
precision_val = precision_score(y_val_numeric, y_val_pred_numeric_tuned_gbc)
recall_val = recall_score(y_val_numeric, y_val_pred_numeric_tuned_gbc)
f1_val = f1_score(y_val_numeric, y_val_pred_numeric_tuned_gbc)

print("\nValidation Set Metrics:")
print(f"  Accuracy: {accuracy_val:.4f}")
print(f"  Precision: {precision_val:.4f}")
print(f"  Recall: {recall_val:.4f}")
print(f"  F1-Score: {f1_val:.4f}")


# Calculate metrics for Test Set
accuracy_test = accuracy_score(y_test_numeric, y_test_pred_numeric_tuned_gbc)
precision_test = precision_score(y_test_numeric, y_test_pred_numeric_tuned_gbc)
recall_test = recall_score(y_test_numeric, y_test_pred_numeric_tuned_gbc)
f1_test = f1_score(y_test_numeric, y_test_pred_numeric_tuned_gbc)

print("\nTest Set Metrics:")
print(f"  Accuracy: {accuracy_test:.4f}")
print(f"  Precision: {precision_test:.4f}")
print(f"  Recall: {recall_test:.4f}")
print(f"  F1-Score: {f1_test:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_tuned_gbc))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_tuned_gbc))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_tuned_gbc))

"""After tuning, the model continued to struggle with identifying the minority class, as shown by persistently low recall in all datasets."""

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = tuned_gbc_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = tuned_gbc_model.predict_proba(X_val)[:, 1]
y_test_proba = tuned_gbc_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the tuned gradient boosting regressor model
feature_importances_tuned_gbc = tuned_gbc_model.feature_importances_

# Get the feature names from the training data
feature_names = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_tuned_gbc = pd.Series(feature_importances_tuned_gbc, index=feature_names)

# Sort the feature importances in descending order
sorted_feature_importances_tuned_gbc = feature_importance_series_tuned_gbc.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Tuned Gradient Boosting Classifier:")
print(sorted_feature_importances_tuned_gbc)

"""# Bootstrap Forest Model

## Iteration 1 - All Features
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Instantiate a Random Forest Classifier model
rf_model = RandomForestClassifier(random_state=42)

# Train the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on training set
y_train_pred_rf = rf_model.predict(X_train)

# Make predictions on validation set
y_val_pred_rf = rf_model.predict(X_val)

# Make predictions on test set
y_test_pred_rf = rf_model.predict(X_test)

"""### Model Assessment Iteration 1"""

from sklearn.metrics import confusion_matrix

# Predictions on training set
cm_train = confusion_matrix(y_train, y_train_pred_rf)

# Predictions on validation set
cm_val = confusion_matrix(y_val, y_val_pred_rf)

# Predictions on test set
cm_test = confusion_matrix(y_test, y_test_pred_rf)

print("Confusion Matrix - Training Set:")
print(cm_train)

print("\nConfusion Matrix - Validation Set:")
print(cm_val)

print("\nConfusion Matrix - Test Set:")
print(cm_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_rf = pd.to_numeric(y_train_pred_rf)
y_val_pred_numeric_rf = pd.to_numeric(y_val_pred_rf)
y_test_pred_numeric_rf = pd.to_numeric(y_test_pred_rf)


# Calculate metrics for Training Set
accuracy_train = accuracy_score(y_train_numeric, y_train_pred_numeric_rf)
precision_train = precision_score(y_train_numeric, y_train_pred_numeric_rf)
recall_train = recall_score(y_train_numeric, y_train_pred_numeric_rf)
f1_train = f1_score(y_train_numeric, y_train_pred_numeric_rf)

print("Training Set Metrics:")
print(f"  Accuracy: {accuracy_train:.4f}")
print(f"  Precision: {precision_train:.4f}")
print(f"  Recall: {recall_train:.4f}")
print(f"  F1-Score: {f1_train:.4f}")

# Calculate metrics for Validation Set
accuracy_val = accuracy_score(y_val_numeric, y_val_pred_numeric_rf)
precision_val = precision_score(y_val_numeric, y_val_pred_numeric_rf)
recall_val = recall_score(y_val_numeric, y_val_pred_numeric_rf)
f1_val = f1_score(y_val_numeric, y_val_pred_numeric_rf)

print("\nValidation Set Metrics:")
print(f"  Accuracy: {accuracy_val:.4f}")
print(f"  Precision: {precision_val:.4f}")
print(f"  Recall: {recall_val:.4f}")
print(f"  F1-Score: {f1_val:.4f}")


# Calculate metrics for Test Set
accuracy_test = accuracy_score(y_test_numeric, y_test_pred_numeric_rf)
precision_test = precision_score(y_test_numeric, y_test_pred_numeric_rf)
recall_test = recall_score(y_test_numeric, y_test_pred_numeric_rf)
f1_test = f1_score(y_test_numeric, y_test_pred_numeric_rf)

print("\nTest Set Metrics:")
print(f"  Accuracy: {accuracy_test:.4f}")
print(f"  Precision: {precision_test:.4f}")
print(f"  Recall: {recall_test:.4f}")
print(f"  F1-Score: {f1_test:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_rf))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_rf))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_rf))

"""The model shows overfitting, performing almost perfectly on the training data while continuing to exhibit poor recall for the minority class on both validation and test sets."""

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = rf_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = rf_model.predict_proba(X_val)[:, 1]
y_test_proba = rf_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the decision tree model
feature_importances_rf = rf_model.feature_importances_

# Get the feature names from the training data (assuming X_train is consistent)
feature_names_rf = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_rf = pd.Series(feature_importances_rf, index=feature_names_rf)

# Sort the feature importances in descending order
sorted_feature_importances_rf = feature_importance_series_rf.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Random Forest:")
print(sorted_feature_importances_rf)

"""## Iteration 2 - Tuning"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Instantiate a Random Forest Classifier model
rf_tuned_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=12,
    min_samples_split=5,
    min_samples_leaf=3,
    random_state=42
)

# Train the model on the training data
rf_tuned_model.fit(X_train, y_train)

# Make predictions on training set
y_train_pred_rf_tuned = rf_tuned_model.predict(X_train)

# Make predictions on validation set
y_val_pred_rf_tuned = rf_tuned_model.predict(X_val)

# Make predictions on test set
y_test_pred_rf_tuned = rf_tuned_model.predict(X_test)

"""Given the long runtime (several hours) of GridSearchCV, we opted to use the strongest hyperparameter set obtained during the search instead of running process again to improve efficiency.

### Model Assessment Iteration 2
"""

from sklearn.metrics import confusion_matrix

# Predictions on training set
cm_train = confusion_matrix(y_train, y_train_pred_rf_tuned)

# Predictions on validation set
cm_val = confusion_matrix(y_val, y_val_pred_rf_tuned)

# Predictions on test set
cm_test = confusion_matrix(y_test, y_test_pred_rf_tuned)

print("Confusion Matrix - Training Set:")
print(cm_train)

print("\nConfusion Matrix - Validation Set:")
print(cm_val)

print("\nConfusion Matrix - Test Set:")
print(cm_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Convert predicted values to numeric type
y_train_pred_numeric_rf_tuned = pd.to_numeric(y_train_pred_rf_tuned)
y_val_pred_numeric_rf_tuned = pd.to_numeric(y_val_pred_rf_tuned)
y_test_pred_numeric_rf_tuned = pd.to_numeric(y_test_pred_rf_tuned)


# Calculate metrics for Training Set
accuracy_train = accuracy_score(y_train_numeric, y_train_pred_numeric_rf_tuned)
precision_train = precision_score(y_train_numeric, y_train_pred_numeric_rf_tuned)
recall_train = recall_score(y_train_numeric, y_train_pred_numeric_rf_tuned)
f1_train = f1_score(y_train_numeric, y_train_pred_numeric_rf_tuned)

print("Training Set Metrics:")
print(f"  Accuracy: {accuracy_train:.4f}")
print(f"  Precision: {precision_train:.4f}")
print(f"  Recall: {recall_train:.4f}")
print(f"  F1-Score: {f1_train:.4f}")

# Calculate metrics for Validation Set
accuracy_val = accuracy_score(y_val_numeric, y_val_pred_numeric_rf_tuned)
precision_val = precision_score(y_val_numeric, y_val_pred_numeric_rf_tuned)
recall_val = recall_score(y_val_numeric, y_val_pred_numeric_rf_tuned)
f1_val = f1_score(y_val_numeric, y_val_pred_numeric_rf_tuned)

print("\nValidation Set Metrics:")
print(f"  Accuracy: {accuracy_val:.4f}")
print(f"  Precision: {precision_val:.4f}")
print(f"  Recall: {recall_val:.4f}")
print(f"  F1-Score: {f1_val:.4f}")


# Calculate metrics for Test Set
accuracy_test = accuracy_score(y_test_numeric, y_test_pred_numeric_rf_tuned)
precision_test = precision_score(y_test_numeric, y_test_pred_numeric_rf_tuned)
recall_test = recall_score(y_test_numeric, y_test_pred_numeric_rf_tuned)
f1_test = f1_score(y_test_numeric, y_test_pred_numeric_rf_tuned)

print("\nTest Set Metrics:")
print(f"  Accuracy: {accuracy_test:.4f}")
print(f"  Precision: {precision_test:.4f}")
print(f"  Recall: {recall_test:.4f}")
print(f"  F1-Score: {f1_test:.4f}")

from sklearn.metrics import classification_report

# Show classification report for the training set
print("Classification Report - Training Set:")
print(classification_report(y_train, y_train_pred_rf_tuned))

# Show classification report for the validation set
print("\nClassification Report - Validation Set:")
print(classification_report(y_val, y_val_pred_rf_tuned))

# Show classification report for the test set
print("\nClassification Report - Test Set:")
print(classification_report(y_test, y_test_pred_rf_tuned))

"""This model predicts the majority class really well, but it still struggles to identify true positive cases, showing strong accuracy but very low recall for diabetes (class 1)."""

# Get predicted probabilities for the training, validation, and test sets
y_train_proba = rf_model.predict_proba(X_train)[:, 1] # Probability of the positive class (1)
y_val_proba = rf_model.predict_proba(X_val)[:, 1]
y_test_proba = rf_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

# Calculate ROC curve and AUC for the training set
fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_proba)
roc_auc_train = auc(fpr_train, tpr_train)

# Calculate ROC curve and AUC for the validation set
fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_proba)
roc_auc_val = auc(fpr_val, tpr_val)

# Calculate ROC curve and AUC for the test set
fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_proba)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (AUC = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (AUC = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing (AUC = 0.50)') # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"Training AUC: {roc_auc_train:.4f}")
print(f"Validation AUC: {roc_auc_val:.4f}")
print(f"Test AUC: {roc_auc_test:.4f}")

import pandas as pd

# Get the feature importances from the decision tree model
feature_importances_rf_tuned = rf_tuned_model.feature_importances_

# Get the feature names from the training data (assuming X_train is consistent)
feature_names_rf_tuned = X_train.columns

# Create a pandas Series to easily view feature importances with their names
feature_importance_series_rf_tuned = pd.Series(feature_importances_rf_tuned, index=feature_names_rf)

# Sort the feature importances in descending order
sorted_feature_importances_rf_tuned = feature_importance_series_rf_tuned.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances for Random Forest (tuned):")
print(sorted_feature_importances_rf_tuned)

"""# Neural Network Model

## Iteration 1 - All Features
"""

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
#from sklearn.metrics import mean_squared_error # Import mean_squared_error to calculate MSE first

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


# Build the neural network model
nn_model = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=200, random_state=42, learning_rate_init=0.01)

# Train the model
nn_model.fit(X_train_scaled, y_train)

"""### Model Assessment Iteration 1"""

from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the training, validation, and test sets
y_train_pred_nn = nn_model.predict(X_train_scaled)
y_val_pred_nn = nn_model.predict(X_val_scaled)
y_test_pred_nn = nn_model.predict(X_test_scaled)

# Print classification reports (includes precision, recall, f1-score, and support)
print("Training Set Classification Report:")
print(classification_report(y_train, y_train_pred_nn))

print("\nValidation Set Classification Report:")
print(classification_report(y_val, y_val_pred_nn))

print("\nTest Set Classification Report:")
print(classification_report(y_test, y_test_pred_nn))


# Calculate confusion matrices
conf_matrix_train = confusion_matrix(y_train, y_train_pred_nn)
conf_matrix_val = confusion_matrix(y_val, y_val_pred_nn)
conf_matrix_test = confusion_matrix(y_test, y_test_pred_nn)

# Calculate accuracy scores
accuracy_train = accuracy_score(y_train, y_train_pred_nn)
accuracy_val = accuracy_score(y_val, y_val_pred_nn)
accuracy_test = accuracy_score(y_test, y_test_pred_nn)


# Plot confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Training Set Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_xticklabels(['0', '1']) # Updated labels
axes[0].set_yticklabels(['0', '1']) # Updated labels


sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Validation Set Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_xticklabels(['0', '1']) # Updated labels
axes[1].set_yticklabels(['0', '1']) # Updated labels


sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', ax=axes[2])
axes[2].set_title('Test Set Confusion Matrix')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_xticklabels(['0', '1']) # Updated labels
axes[2].set_yticklabels(['0', '1']) # Updated labels

plt.tight_layout()
plt.show()


# Calculate ROC curves and AUC
y_train_prob = nn_model.predict_proba(X_train)[:, 1]
y_val_prob = nn_model.predict_proba(X_val)[:, 1]
y_test_prob = nn_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)


fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_prob)
roc_auc_train = auc(fpr_train, tpr_train)

fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_prob)
roc_auc_val = auc(fpr_val, tpr_val)

fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_prob)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""The model shows slightly higher recall for the minority class compared to previous models, but recall remains low across all sets, indicating continued difficulty in detecting class 1. So we tried the model with oversampling to improve the overall performance.

## Iteration 2 - Oversampling
"""

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the scaled training data
X_train_oversampled_nn, y_train_oversampled_nn = smote.fit_resample(X_train_scaled, y_train)

# Print the shape of the original and oversampled target variables
print("Original y_train shape:", y_train.shape)
print("Oversampled y_train_oversampled_nn shape:", y_train_oversampled_nn.shape)
print("Class distribution after SMOTE:\n", pd.Series(y_train_oversampled_nn).value_counts())

nn_model_oversampled = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=200, random_state=42, learning_rate_init=0.01)
nn_model_oversampled.fit(X_train_oversampled_nn, y_train_oversampled_nn)

print("Neural Network model trained successfully with oversampled data.")

"""### Model Assessment Iteration 2"""

from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the training, validation, and test sets
y_train_pred_nn_oversampled = nn_model_oversampled.predict(X_train_oversampled_nn)
y_val_pred_nn_oversampled = nn_model_oversampled.predict(X_val_scaled)
y_test_pred_nn_oversampled = nn_model_oversampled.predict(X_test_scaled)

# Print classification reports
print("\nOversampled Training Set Classification Report:")
print(classification_report(y_train_oversampled_nn, y_train_pred_nn_oversampled))

print("\nValidation Set Classification Report:")
print(classification_report(y_val, y_val_pred_nn_oversampled))

print("\nTest Set Classification Report:")
print(classification_report(y_test, y_test_pred_nn_oversampled))

# Calculate confusion matrices
conf_matrix_train_oversampled_nn = confusion_matrix(y_train_oversampled_nn, y_train_pred_nn_oversampled)
conf_matrix_val_nn_oversampled = confusion_matrix(y_val, y_val_pred_nn_oversampled)
conf_matrix_test_nn_oversampled = confusion_matrix(y_test, y_test_pred_nn_oversampled)

# Plot confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(conf_matrix_train_oversampled_nn, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Oversampled Training Set Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_xticklabels(['0', '1'])
axes[0].set_yticklabels(['0', '1'])

sns.heatmap(conf_matrix_val_nn_oversampled, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Validation Set Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_xticklabels(['0', '1'])
axes[1].set_yticklabels(['0', '1'])

sns.heatmap(conf_matrix_test_nn_oversampled, annot=True, fmt='d', cmap='Blues', ax=axes[2])
axes[2].set_title('Test Set Confusion Matrix')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_xticklabels(['0', '1'])
axes[2].set_yticklabels(['0', '1'])

plt.tight_layout()
plt.show()

# Calculate ROC curves and AUC
y_train_prob_nn_oversampled = nn_model_oversampled.predict_proba(X_train_oversampled_nn)[:, 1]
y_val_prob_nn_oversampled = nn_model_oversampled.predict_proba(X_val_scaled)[:, 1]
y_test_prob_nn_oversampled = nn_model_oversampled.predict_proba(X_test_scaled)[:, 1]

# Convert y_train_oversampled_nn to numeric before passing to roc_curve
y_train_oversampled_nn_numeric = pd.to_numeric(y_train_oversampled_nn)

fpr_train_nn_oversampled, tpr_train_nn_oversampled, _ = roc_curve(y_train_oversampled_nn_numeric, y_train_prob_nn_oversampled)
roc_auc_train_nn_oversampled = auc(fpr_train_nn_oversampled, tpr_train_nn_oversampled)

fpr_val_nn_oversampled, tpr_val_nn_oversampled, _ = roc_curve(y_val_numeric, y_val_prob_nn_oversampled)
roc_auc_val_nn_oversampled = auc(fpr_val_nn_oversampled, tpr_val_nn_oversampled)

fpr_test_nn_oversampled, tpr_test_nn_oversampled, _ = roc_curve(y_test_numeric, y_test_prob_nn_oversampled)
roc_auc_test_nn_oversampled = auc(fpr_test_nn_oversampled, tpr_test_nn_oversampled)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train_nn_oversampled, tpr_train_nn_oversampled, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train_nn_oversampled)
plt.plot(fpr_val_nn_oversampled, tpr_val_nn_oversampled, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val_nn_oversampled)
plt.plot(fpr_test_nn_oversampled, tpr_test_nn_oversampled, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test_nn_oversampled)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (NN Oversampled Model)')
plt.legend(loc="lower right")
plt.show()

print(f"\nTraining AUC (NN Oversampled Model): {roc_auc_train_nn_oversampled:.4f}")
print(f"Validation AUC (NN Oversampled Model): {roc_auc_val_nn_oversampled:.4f}")
print(f"Test AUC (NN Oversampled Model): {roc_auc_test_nn_oversampled:.4f}")

"""After applying oversampling, the model significantly improved minority-class recall on the validation and test sets, though this came at the cost of lower precision for class 1 and reduced overall accuracy.

# Discriminant Analysis Model

## Iteration 1 - All Features
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Initialize the LDA model
lda_model = LinearDiscriminantAnalysis()

# Train the model using the training data
lda_model.fit(X_train, y_train)

print("LDA model trained successfully.")

"""### Model Assessment Iteration 1"""

from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the training, validation, and test sets
y_train_pred_lda = lda_model.predict(X_train)
y_val_pred_lda = lda_model.predict(X_val)
y_test_pred_lda = lda_model.predict(X_test)
# Print classification reports (includes precision, recall, f1-score, and support)
print("Training Set Classification Report:")
print(classification_report(y_train, y_train_pred_lda))

print("\nValidation Set Classification Report:")
print(classification_report(y_val, y_val_pred_lda))

print("\nTest Set Classification Report:")
print(classification_report(y_test, y_test_pred_lda))


# Calculate confusion matrices
conf_matrix_train = confusion_matrix(y_train, y_train_pred_lda)
conf_matrix_val = confusion_matrix(y_val, y_val_pred_lda)
conf_matrix_test = confusion_matrix(y_test, y_test_pred_lda)

# Calculate accuracy scores
accuracy_train = accuracy_score(y_train, y_train_pred_lda)
accuracy_val = accuracy_score(y_val, y_val_pred_lda)
accuracy_test = accuracy_score(y_test, y_test_pred_lda)


# Plot confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Training Set Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_xticklabels(['0', '1']) # Updated labels
axes[0].set_yticklabels(['0', '1']) # Updated labels


sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Validation Set Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_xticklabels(['0', '1']) # Updated labels
axes[1].set_yticklabels(['0', '1']) # Updated labels


sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', ax=axes[2])
axes[2].set_title('Test Set Confusion Matrix')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_xticklabels(['0', '1']) # Updated labels
axes[2].set_yticklabels(['0', '1']) # Updated labels

plt.tight_layout()
plt.show()


# Calculate ROC curves and AUC
y_train_prob = lda_model.predict_proba(X_train)[:, 1]
y_val_prob = lda_model.predict_proba(X_val)[:, 1]
y_test_prob = lda_model.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)


fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_prob)
roc_auc_train = auc(fpr_train, tpr_train)

fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_prob)
roc_auc_val = auc(fpr_val, tpr_val)

fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_prob)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""Although the model generalizes well with similar performance across all datasets, it still exhibits quite poor recall for the minority class, indicating difficulty separating class 1 from class 0."""

# Get the feature names from X_train
feature_names = X_train.columns

# Get the coefficients from the trained LDA model
coefficients = lda_model.coef_[0]

# Create a pandas Series to associate coefficients with feature names
coef_series = pd.Series(coefficients, index=feature_names)

# Display the coefficients with predictor names
print("Discriminant Function Coefficients:")
print(coef_series)
print("\nIntercept:", lda_model.intercept_[0])

"""# KNN Model Iteration 1

## Iteration 1 - Default K
"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn_classifier = KNeighborsClassifier()

# Train the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_val_knn = knn_classifier.predict(X_val)

"""### Model Assessment Iteration 1"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd # Import pandas if not already imported in this cell

# Evaluate on the training set
y_pred_train_knn = knn_classifier.predict(X_train)

# Convert y_train and y_pred_train_knn to numeric for metric calculation
y_train_numeric = pd.to_numeric(y_train)
y_pred_train_knn_numeric = pd.to_numeric(y_pred_train_knn)

accuracy_train = accuracy_score(y_train_numeric, y_pred_train_knn_numeric)
precision_train = precision_score(y_train_numeric, y_pred_train_knn_numeric)
recall_train = recall_score(y_train_numeric, y_pred_train_knn_numeric)
f1_train = f1_score(y_train_numeric, y_pred_train_knn_numeric)

print("Performance metrics for Training Set:")
print(f"Accuracy: {accuracy_train:.4f}")
print(f"Precision: {precision_train:.4f}")
print(f"Recall: {recall_train:.4f}")
print(f"F1-score: {f1_train:.4f}")

# Evaluate on the validation set (predictions already made)
# Convert y_val and y_pred_val_knn to numeric for metric calculation
y_val_numeric = pd.to_numeric(y_val)
y_pred_val_knn_numeric = pd.to_numeric(y_pred_val_knn)

accuracy_val = accuracy_score(y_val_numeric, y_pred_val_knn_numeric)
precision_val = precision_score(y_val_numeric, y_pred_val_knn_numeric)
recall_val = recall_score(y_val_numeric, y_pred_val_knn_numeric)
f1_val = f1_score(y_val_numeric, y_pred_val_knn_numeric)

print("\nPerformance metrics for Validation Set:")
print(f"Accuracy: {accuracy_val:.4f}")
print(f"Precision: {precision_val:.4f}")
print(f"Recall: {recall_val:.4f}")
print(f"F1-score: {f1_val:.4f}")

# Evaluate on the test set
y_pred_test_knn = knn_classifier.predict(X_test)

# Convert y_test and y_pred_test_knn to numeric for metric calculation
y_test_numeric = pd.to_numeric(y_test)
y_pred_test_knn_numeric = pd.to_numeric(y_pred_test_knn)

accuracy_test = accuracy_score(y_test_numeric, y_pred_test_knn_numeric)
precision_test = precision_score(y_test_numeric, y_pred_test_knn_numeric)
recall_test = recall_score(y_test_numeric, y_pred_test_knn_numeric)
f1_test = f1_score(y_test_numeric, y_pred_test_knn_numeric)

print("\nPerformance metrics for Test Set:")
print(f"Accuracy: {accuracy_test:.4f}")
print(f"Precision: {precision_test:.4f}")
print(f"Recall: {recall_test:.4f}")
print(f"F1-score: {f1_test:.4f}")

# Print classification reports (includes precision, recall, f1-score, and support)
print("Training Set Classification Report:")
print(classification_report(y_train, y_pred_train_knn))

print("\nValidation Set Classification Report:")
print(classification_report(y_val, y_pred_val_knn))

print("\nTest Set Classification Report:")
print(classification_report(y_test, y_pred_test_knn))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Create a figure with three subplots side by side
fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # 1 row, 3 columns

# Confusion matrix for training set
sns.heatmap(confusion_matrix(y_train, y_pred_train_knn), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[0])
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_title('Confusion Matrix - Training Set')

# Confusion matrix for validation set
sns.heatmap(confusion_matrix(y_val, y_pred_val_knn), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[1])
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_title('Confusion Matrix - Validation Set')

# Confusion matrix for test set
sns.heatmap(confusion_matrix(y_test, y_pred_test_knn), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[2])
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_title('Confusion Matrix - Test Set')

plt.tight_layout() # Adjust layout to prevent overlapping titles/labels
plt.show()

# Calculate ROC curves and AUC
y_train_prob = knn_classifier.predict_proba(X_train)[:, 1]
y_val_prob = knn_classifier.predict_proba(X_val)[:, 1]
y_test_prob = knn_classifier.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)


fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_prob)
roc_auc_train = auc(fpr_train, tpr_train)

fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_prob)
roc_auc_val = auc(fpr_val, tpr_val)

fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_prob)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""Although with default K, the model maintains similar accuracy across all datasets, it exhibits a substantial decline in minority class recall in validation and test set, indicating limited ability to generalize the class of interest and a mild overfiting.

## Iteration 2 - Optimal K
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define a range of n_neighbors values to try
n_neighbors_range = [1,5,10,15,20] # Example range from 1 to 20

# Create lists to store results
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Convert y_val to numeric type for metric calculation
y_val_numeric = pd.to_numeric(y_val)

# Iterate through the range of n_neighbors values
for n in n_neighbors_range:
    # Initialize the KNN classifier with the current n_neighbors
    knn_classifier = KNeighborsClassifier(n_neighbors=n)

    # Train the model
    knn_classifier.fit(X_train, y_train)

    # Make predictions on the validation set
    y_pred_val = knn_classifier.predict(X_val)

    # Evaluate the model on the validation set
    accuracy = accuracy_score(y_val_numeric, pd.to_numeric(y_pred_val))
    precision = precision_score(y_val_numeric, pd.to_numeric(y_pred_val))
    recall = recall_score(y_val_numeric, pd.to_numeric(y_pred_val))
    f1 = f1_score(y_val_numeric, pd.to_numeric(y_pred_val))

    # Store the scores
    accuracy_scores.append(accuracy)
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)

# Plot the performance metrics for each n_neighbors
plt.figure(figsize=(12, 8))

plt.plot(n_neighbors_range, accuracy_scores, label='Accuracy')
plt.plot(n_neighbors_range, precision_scores, label='Precision')
plt.plot(n_neighbors_range, recall_scores, label='Recall')
plt.plot(n_neighbors_range, f1_scores, label='F1-score')

plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Score')
plt.title('KNN Performance on Validation Set for Different n_neighbors')
plt.xticks(n_neighbors_range)
plt.legend()
plt.grid(True)
plt.show()

"""Since 5 is the default k and k = 1 usually casuses overfits, experimenting with k=15."""

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier with n_neighbors=5
knn_classifier_15 = KNeighborsClassifier(n_neighbors=15)

# Train the model
knn_classifier_15.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_val_15 = knn_classifier_15.predict(X_val)

"""### Model Assessment Iteration 2"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd # Import pandas if not already imported in this cell

# Evaluate on the training set
y_pred_train_15 = knn_classifier_15.predict(X_train)

# Convert y_train and y_pred_train_15 to numeric for metric calculation
y_train_numeric = pd.to_numeric(y_train)
y_pred_train_15_numeric = pd.to_numeric(y_pred_train_15)

accuracy_train = accuracy_score(y_train_numeric, y_pred_train_15_numeric)
precision_train = precision_score(y_train_numeric, y_pred_train_15_numeric)
recall_train = recall_score(y_train_numeric, y_pred_train_15_numeric)
f1_train = f1_score(y_train_numeric, y_pred_train_15_numeric)

print("Performance metrics for Training Set (n_neighbors=15):")
print(f"Accuracy: {accuracy_train:.4f}")
print(f"Precision: {precision_train:.4f}")
print(f"Recall: {recall_train:.4f}")
print(f"F1-score: {f1_train:.4f}")

# Evaluate on the validation set (predictions already made)
# Convert y_val and y_pred_val_15 to numeric for metric calculation
y_val_numeric = pd.to_numeric(y_val)
y_pred_val_15_numeric = pd.to_numeric(y_pred_val_15)

accuracy_val = accuracy_score(y_val_numeric, y_pred_val_15_numeric)
precision_val = precision_score(y_val_numeric, y_pred_val_15_numeric)
recall_val = recall_score(y_val_numeric, y_pred_val_15_numeric)
f1_val = f1_score(y_val_numeric, y_pred_val_15_numeric)

print("\nPerformance metrics for Validation Set (n_neighbors=15):")
print(f"Accuracy: {accuracy_val:.4f}")
print(f"Precision: {precision_val:.4f}")
print(f"Recall: {recall_val:.4f}")
print(f"F1-score: {f1_val:.4f}")

# Evaluate on the test set
y_pred_test_15 = knn_classifier_15.predict(X_test)

# Convert y_test and y_pred_test_15 to numeric for metric calculation
y_test_numeric = pd.to_numeric(y_test)
y_pred_test_15_numeric = pd.to_numeric(y_pred_test_15)

accuracy_test = accuracy_score(y_test_numeric, y_pred_test_15_numeric)
precision_test = precision_score(y_test_numeric, y_pred_test_15_numeric)
recall_test = recall_score(y_test_numeric, y_pred_test_15_numeric)
f1_test = f1_score(y_test_numeric, y_pred_test_15_numeric)

print("\nPerformance metrics for Test Set (n_neighbors=15):")
print(f"Accuracy: {accuracy_test:.4f}")
print(f"Precision: {precision_test:.4f}")
print(f"Recall: {recall_test:.4f}")
print(f"F1-score: {f1_test:.4f}")

# Print classification reports (includes precision, recall, f1-score, and support)
print("Training Set Classification Report:")
print(classification_report(y_train, y_pred_train_15))

print("\nValidation Set Classification Report:")
print(classification_report(y_val, y_pred_val_15))

print("\nTest Set Classification Report:")
print(classification_report(y_test, y_pred_test_15))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Create a figure with three subplots side by side
fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # 1 row, 3 columns

# Confusion matrix for training set
sns.heatmap(confusion_matrix(y_train, y_pred_train_15), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[0])
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_title('Confusion Matrix - Training Set (n_neighbors=15)')

# Confusion matrix for validation set
sns.heatmap(confusion_matrix(y_val, y_pred_val_15), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[1])
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_title('Confusion Matrix - Validation Set (n_neighbors=15)')

# Confusion matrix for test set
sns.heatmap(confusion_matrix(y_test, y_pred_test_15), annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'], ax=axes[2])
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_title('Confusion Matrix - Test Set (n_neighbors=15)')

plt.tight_layout() # Adjust layout to prevent overlapping titles/labels
plt.show()

# Calculate ROC curves and AUC
y_train_prob = knn_classifier_15.predict_proba(X_train)[:, 1]
y_val_prob = knn_classifier_15.predict_proba(X_val)[:, 1]
y_test_prob = knn_classifier_15.predict_proba(X_test)[:, 1]

# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)


fpr_train, tpr_train, thresholds_train = roc_curve(y_train_numeric, y_train_prob)
roc_auc_train = auc(fpr_train, tpr_train)

fpr_val, tpr_val, thresholds_val = roc_curve(y_val_numeric, y_val_prob)
roc_auc_val = auc(fpr_val, tpr_val)

fpr_test, tpr_test, thresholds_test = roc_curve(y_test_numeric, y_test_prob)
roc_auc_test = auc(fpr_test, tpr_test)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train)
plt.plot(fpr_val, tpr_val, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val)
plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""# Naive Bayes Model

## Iteration 1 - All Features
"""

from sklearn.naive_bayes import GaussianNB

# Initialize the Gaussian Naive Bayes model
gnb_model = GaussianNB()

# Train the model using the training data
gnb_model.fit(X_train, y_train)

print("Gaussian Naive Bayes model trained successfully.")

"""### Model Assessment Iteration 1"""

from sklearn.metrics import classification_report

# Make predictions on the training, validation, and test sets
y_train_pred_gnb = gnb_model.predict(X_train)
y_val_pred_gnb = gnb_model.predict(X_val)
y_test_pred_gnb = gnb_model.predict(X_test)

# Print the classification reports
print("Training Set Classification Report (Naive Bayes):")
print(classification_report(y_train, y_train_pred_gnb))

print("\nValidation Set Classification Report (Naive Bayes):")
print(classification_report(y_val, y_val_pred_gnb))

print("\nTest Set Classification Report (Naive Bayes):")
print(classification_report(y_test, y_test_pred_gnb))

"""The Naive Bayes model shows stable generalization across all datasets with quite strong recall for the minority class, but its low precision makes it prone to producing many false positives."""

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions and get probabilities on the training, validation, and test sets
y_train_pred_gnb = gnb_model.predict(X_train)
y_val_pred_gnb = gnb_model.predict(X_val)
y_test_pred_gnb = gnb_model.predict(X_test)

y_train_prob_gnb = gnb_model.predict_proba(X_train)[:, 1]
y_val_prob_gnb = gnb_model.predict_proba(X_val)[:, 1]
y_test_prob_gnb = gnb_model.predict_proba(X_test)[:, 1]


# Calculate confusion matrices
conf_matrix_train_gnb = confusion_matrix(y_train, y_train_pred_gnb)
conf_matrix_val_gnb = confusion_matrix(y_val, y_val_pred_gnb)
conf_matrix_test_gnb = confusion_matrix(y_test, y_test_pred_gnb)

# Plot confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.heatmap(conf_matrix_train_gnb, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Training Set Confusion Matrix (Naive Bayes)')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')
axes[0].set_xticklabels(['0', '1']) # Corrected labels
axes[0].set_yticklabels(['0', '1']) # Corrected labels


sns.heatmap(conf_matrix_val_gnb, annot=True, fmt='d', cmap='Blues', ax=axes[1])
axes[1].set_title('Validation Set Confusion Matrix (Naive Bayes)')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
axes[1].set_xticklabels(['0', '1']) # Corrected labels
axes[1].set_yticklabels(['0', '1']) # Corrected labels


sns.heatmap(conf_matrix_test_gnb, annot=True, fmt='d', cmap='Blues', ax=axes[2])
axes[2].set_title('Test Set Confusion Matrix (Naive Bayes)')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('Actual')
axes[2].set_xticklabels(['0', '1']) # Corrected labels
axes[2].set_yticklabels(['0', '1']) # Corrected labels

plt.tight_layout()
plt.show()


# Calculate ROC curves and AUC
# Convert y_train, y_val, and y_test to numeric type
y_train_numeric = pd.to_numeric(y_train)
y_val_numeric = pd.to_numeric(y_val)
y_test_numeric = pd.to_numeric(y_test)

fpr_train_gnb, tpr_train_gnb, thresholds_train_gnb = roc_curve(y_train_numeric, y_train_prob_gnb)
roc_auc_train_gnb = auc(fpr_train_gnb, tpr_train_gnb)

fpr_val_gnb, tpr_val_gnb, thresholds_val_gnb = roc_curve(y_val_numeric, y_val_prob_gnb)
roc_auc_val_gnb = auc(fpr_val_gnb, tpr_val_gnb)

fpr_test_gnb, tpr_test_gnb, thresholds_test_gnb = roc_curve(y_test_numeric, y_test_prob_gnb)
roc_auc_test_gnb = auc(fpr_test_gnb, tpr_test_gnb)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_train_gnb, tpr_train_gnb, color='darkorange', lw=2, label='Training ROC curve (area = %0.2f)' % roc_auc_train_gnb)
plt.plot(fpr_val_gnb, tpr_val_gnb, color='green', lw=2, label='Validation ROC curve (area = %0.2f)' % roc_auc_val_gnb)
plt.plot(fpr_test_gnb, tpr_test_gnb, color='blue', lw=2, label='Test ROC curve (area = %0.2f)' % roc_auc_test_gnb)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Naive Bayes)')
plt.legend(loc="lower right")
plt.show()